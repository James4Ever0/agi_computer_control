{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multimodal Transformer based on GPT2","metadata":{}},{"cell_type":"markdown","source":"## Load the GPT2 model","metadata":{}},{"cell_type":"code","source":"import transformers\n\n\nmodel_name = \"distilgpt2\"\nmodel = transformers.AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n# outputs = model(**inputs, labels=inputs[\"input_ids\"])\n# loss = outputs.loss\n\nprint(model)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-27T05:48:17.535032Z","iopub.execute_input":"2024-02-27T05:48:17.535485Z","iopub.status.idle":"2024-02-27T05:48:24.311804Z","shell.execute_reply.started":"2024-02-27T05:48:17.535452Z","shell.execute_reply":"2024-02-27T05:48:24.310782Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce5d8ad26fef40de93429b98c4cca56f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a700118a71d4f728ef2d4358d1c1516"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f57715693dd4c06801b32ff031db803"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f76e2376c7f949419b0764d3cb9d446a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0f4e1e085b243828a2b6171d9a7a48a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e070d32603c47e79eb162dc9ae8b2b9"}},"metadata":{}},{"name":"stdout","text":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Encode tokens into embeddings","metadata":{}},{"cell_type":"code","source":"# dir(model)\n\ninput_ids = inputs['input_ids'] # 6 tokens\ninput_embeds = model.transformer.wte(input_ids)\n\n# print(\"embed_dim:\",model.base_model.embed_dim) # 768\n# model(inputs_embeds=input_embeds, attention_mask=inputs['attention_mask'], labels=inputs['input_ids'])\n\nprint(inputs['input_ids'].shape) # [1,6]\nprint(input_embeds.shape) # [1,6,768]","metadata":{"execution":{"iopub.status.busy":"2024-02-27T07:01:29.218039Z","iopub.execute_input":"2024-02-27T07:01:29.218826Z","iopub.status.idle":"2024-02-27T07:01:29.228850Z","shell.execute_reply.started":"2024-02-27T07:01:29.218692Z","shell.execute_reply":"2024-02-27T07:01:29.227474Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"torch.Size([1, 6])\ntorch.Size([1, 6, 768])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Generate video embeddings","metadata":{}},{"cell_type":"code","source":"from transformers import AutoImageProcessor, ViTModel\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\nmodel = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n\ninputs = image_processor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\nlast_hidden_states = outputs.last_hidden_state\nlist(last_hidden_states.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T05:29:38.327184Z","iopub.execute_input":"2024-03-01T05:29:38.327603Z","iopub.status.idle":"2024-03-01T05:29:40.302271Z","shell.execute_reply.started":"2024-03-01T05:29:38.327574Z","shell.execute_reply":"2024-03-01T05:29:40.301116Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a623e5bc4464a16bf8ef0b95005e883"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[1, 197, 768]"},"metadata":{}}]},{"cell_type":"code","source":"type(image)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T05:41:35.745612Z","iopub.execute_input":"2024-03-01T05:41:35.746011Z","iopub.status.idle":"2024-03-01T05:41:35.754156Z","shell.execute_reply.started":"2024-03-01T05:41:35.745983Z","shell.execute_reply":"2024-03-01T05:41:35.752769Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"PIL.JpegImagePlugin.JpegImageFile"},"metadata":{}}]},{"cell_type":"code","source":"random_image = torch.rand(3, 224, 224)\nimage_processor(random_image, do_rescale=False, return_tensors=\"pt\") # from official doc.","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:08:45.625887Z","iopub.execute_input":"2024-03-01T06:08:45.626416Z","iopub.status.idle":"2024-03-01T06:08:45.654647Z","shell.execute_reply.started":"2024-03-01T06:08:45.626358Z","shell.execute_reply":"2024-03-01T06:08:45.653759Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"{'pixel_values': tensor([[[[ 0.6471,  0.8980, -0.2000,  ...,  0.6314, -0.3412, -0.7255],\n          [-0.3333,  0.4667,  0.5294,  ...,  0.4196,  0.6314,  0.7020],\n          [ 0.8196, -0.4039, -0.8745,  ...,  0.5922,  0.2314, -0.8275],\n          ...,\n          [-0.9373, -0.5451,  0.2941,  ...,  0.2314, -0.4039,  0.1922],\n          [-0.9373, -0.5922, -0.4902,  ..., -0.3412,  0.6235, -0.3569],\n          [-0.7882,  0.5373,  0.3882,  ...,  0.6471, -0.3333,  0.8667]],\n\n         [[ 0.3569,  0.0196, -0.7725,  ..., -0.5137,  0.4824,  0.2000],\n          [-0.8196, -0.0980,  0.1451,  ..., -0.6549, -0.6157,  0.6471],\n          [ 0.2863,  0.3882, -0.9765,  ..., -0.2784,  0.1294, -0.5529],\n          ...,\n          [-0.6784, -0.0196, -0.5059,  ..., -0.8353, -0.7176,  0.3333],\n          [-0.3098, -0.5373, -0.4667,  ..., -0.9843, -0.7333,  0.1765],\n          [-0.2863, -0.0667, -0.6471,  ...,  0.8745, -0.3804, -0.8039]],\n\n         [[ 0.3255,  0.1373, -0.3647,  ...,  0.4196,  0.1294, -0.4431],\n          [-0.5608, -0.0353,  0.4353,  ..., -0.0510, -0.0039, -0.8275],\n          [ 0.0745,  0.3804, -0.4510,  ...,  0.8196, -0.6078,  0.9765],\n          ...,\n          [ 0.2000,  0.8275, -0.6471,  ...,  0.9765, -0.9059, -0.9451],\n          [ 0.8745, -0.9765, -0.2863,  ..., -0.6392, -0.6392, -0.0275],\n          [ 0.0196, -0.6078,  0.2314,  ...,  0.1765,  0.2314, -0.4353]]]])}"},"metadata":{}}]},{"cell_type":"code","source":"import numpy\nimport numpy as np\n# torch_random_image = numpy.random.rand(3, 224, 224)\ntorch_random_image = np.random.randint(0, 256, (224, 224, 3), dtype=numpy.uint8)\n\n# torch_random_image\n\n# 480, 640, 3, uint8\n# print(image_processor(torch_random_image,return_tensors=\"pt\")['pixel_values'].shape) # negative values\n# print(image_processor(torch_random_image,return_tensors=\"pt\")['pixel_values'].mean())\n# print(torch_random_image.mean())\n# model(**image_processor(torch_random_image,return_tensors=\"pt\"))\nnumpy.array(image).shape, image_processor(torch_random_image, return_tensors=\"pt\"), torch_random_image, image_processor(image)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:05:57.627470Z","iopub.execute_input":"2024-03-01T06:05:57.627868Z","iopub.status.idle":"2024-03-01T06:05:57.671134Z","shell.execute_reply.started":"2024-03-01T06:05:57.627839Z","shell.execute_reply":"2024-03-01T06:05:57.670153Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"((480, 640, 3),\n {'pixel_values': tensor([[[[ 0.0824,  0.8196,  0.6000,  ...,  0.1843,  0.4118, -0.1216],\n           [-0.2706, -0.2078,  0.4275,  ...,  0.1686,  0.0275, -0.8667],\n           [-0.8588,  0.3176,  0.0902,  ...,  0.6706,  0.5529,  0.7725],\n           ...,\n           [ 0.3961,  0.1137,  0.0118,  ..., -0.4431, -0.5059,  0.6078],\n           [-0.3176,  0.7020,  0.7255,  ..., -0.3882, -0.7725,  0.7569],\n           [-0.2627, -0.9373,  0.2706,  ...,  0.7882, -0.5373, -0.4196]],\n \n          [[ 0.8667,  0.5294, -0.7255,  ...,  0.8353, -0.7647,  0.4039],\n           [ 0.3961, -0.2784,  0.2863,  ..., -0.6392, -0.3020,  0.8431],\n           [-0.6627,  0.2941,  0.1922,  ...,  0.2471, -0.2000,  0.5765],\n           ...,\n           [-0.4588,  0.2314,  0.7412,  ...,  0.0588, -0.2157, -0.9294],\n           [ 0.2784,  0.8745, -0.9765,  ..., -0.8902,  0.1137, -0.9373],\n           [ 0.6706,  0.6627,  0.4275,  ..., -0.3725, -0.5843, -0.7569]],\n \n          [[-0.0588, -0.9451,  0.6314,  ..., -0.1137,  0.1922,  0.7098],\n           [ 0.6549, -0.3176, -0.4353,  ...,  0.2000, -0.6941, -0.1059],\n           [-0.8275,  0.3569, -0.6706,  ..., -0.0510,  0.5137, -0.1294],\n           ...,\n           [ 0.8902,  0.6157, -0.3333,  ...,  0.5137,  0.4588,  0.8118],\n           [ 0.8980,  0.3882,  0.1059,  ..., -0.6392,  0.4824,  0.1137],\n           [-0.4118,  0.9608,  1.0000,  ...,  0.2000, -0.3176,  0.1686]]]])},\n array([[[138, 238, 120],\n         [232, 195,   7],\n         [204,  35, 208],\n         ...,\n         [151, 234, 113],\n         [180,  30, 152],\n         [112, 179, 218]],\n \n        [[ 93, 178, 211],\n         [101,  92,  87],\n         [182, 164,  72],\n         ...,\n         [149,  46, 153],\n         [131,  89,  39],\n         [ 17, 235, 114]],\n \n        [[ 18,  43,  22],\n         [168, 165, 173],\n         [139, 152,  42],\n         ...,\n         [213, 159, 121],\n         [198, 102, 193],\n         [226, 201, 111]],\n \n        ...,\n \n        [[178,  69, 241],\n         [142, 157, 206],\n         [129, 222,  85],\n         ...,\n         [ 71, 135, 193],\n         [ 63, 100, 186],\n         [205,   9, 231]],\n \n        [[ 87, 163, 242],\n         [217, 239, 177],\n         [220,   3, 141],\n         ...,\n         [ 78,  14,  46],\n         [ 29, 142, 189],\n         [224,   8, 142]],\n \n        [[ 94, 213,  75],\n         [  8, 212, 250],\n         [162, 182, 255],\n         ...,\n         [228,  80, 153],\n         [ 59,  53,  87],\n         [ 74,  31, 149]]], dtype=uint8),\n {'pixel_values': [array([[[ 0.11372554,  0.1686275 ,  0.18431377, ..., -0.19215685,\n          -0.18431371, -0.18431371],\n         [ 0.13725495,  0.1686275 ,  0.18431377, ..., -0.19215685,\n          -0.19215685, -0.20784312],\n         [ 0.11372554,  0.15294123,  0.16078436, ..., -0.23137254,\n          -0.2235294 , -0.21568626],\n         ...,\n         [ 0.8352941 ,  0.7882353 ,  0.73333335, ...,  0.7019608 ,\n           0.64705884,  0.6156863 ],\n         [ 0.827451  ,  0.79607844,  0.77254903, ...,  0.58431375,\n           0.4666667 ,  0.39607847],\n         [ 0.81960785,  0.75686276,  0.75686276, ...,  0.07450986,\n          -0.05098039, -0.19215685]],\n \n        [[-0.8039216 , -0.8117647 , -0.8117647 , ..., -0.8901961 ,\n          -0.8901961 , -0.8980392 ],\n         [-0.7882353 , -0.7882353 , -0.7882353 , ..., -0.8745098 ,\n          -0.8745098 , -0.88235295],\n         [-0.8117647 , -0.8039216 , -0.7882353 , ..., -0.8901961 ,\n          -0.8901961 , -0.8901961 ],\n         ...,\n         [-0.27058822, -0.31764704, -0.36470586, ..., -0.42745095,\n          -0.4588235 , -0.4823529 ],\n         [-0.27058822, -0.29411763, -0.34117645, ..., -0.4823529 ,\n          -0.54509807, -0.5764706 ],\n         [-0.27843136, -0.34117645, -0.3490196 , ..., -0.73333335,\n          -0.78039217, -0.8352941 ]],\n \n        [[-0.54509807, -0.46666664, -0.4823529 , ..., -0.7411765 ,\n          -0.69411767, -0.7176471 ],\n         [-0.5529412 , -0.5137255 , -0.49019605, ..., -0.7411765 ,\n          -0.70980394, -0.7411765 ],\n         [-0.52156866, -0.4823529 , -0.46666664, ..., -0.7490196 ,\n          -0.7490196 , -0.7647059 ],\n         ...,\n         [ 0.5686275 ,  0.5529412 ,  0.45098042, ...,  0.4431373 ,\n           0.38823533,  0.32549024],\n         [ 0.54509807,  0.4901961 ,  0.5137255 , ...,  0.30196083,\n           0.20784318,  0.12941182],\n         [ 0.5686275 ,  0.56078434,  0.5137255 , ..., -0.19999999,\n          -0.42745095, -0.5294118 ]]], dtype=float32)]})"},"metadata":{}}]},{"cell_type":"code","source":"print([key for key in dir(outputs) if not key.startswith('_')])","metadata":{"execution":{"iopub.status.busy":"2024-03-01T05:26:02.326625Z","iopub.execute_input":"2024-03-01T05:26:02.327377Z","iopub.status.idle":"2024-03-01T05:26:02.333979Z","shell.execute_reply.started":"2024-03-01T05:26:02.327333Z","shell.execute_reply":"2024-03-01T05:26:02.332786Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"['attentions', 'clear', 'copy', 'fromkeys', 'get', 'hidden_states', 'items', 'keys', 'last_hidden_state', 'move_to_end', 'pooler_output', 'pop', 'popitem', 'setdefault', 'to_tuple', 'update', 'values']\n","output_type":"stream"}]},{"cell_type":"code","source":"# outputs.attentions # attention tensor per layer.\noutputs.attentions[-1].shape # [1, 12, 197, 197]","metadata":{"execution":{"iopub.status.busy":"2024-03-01T05:27:59.975836Z","iopub.execute_input":"2024-03-01T05:27:59.976229Z","iopub.status.idle":"2024-03-01T05:27:59.983268Z","shell.execute_reply.started":"2024-03-01T05:27:59.976192Z","shell.execute_reply":"2024-03-01T05:27:59.982170Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 12, 197, 197])"},"metadata":{}}]},{"cell_type":"code","source":"outputs.hidden_states[-1].shape # [1, 197, 768]","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:34:02.495937Z","iopub.execute_input":"2024-03-04T10:34:02.496323Z","iopub.status.idle":"2024-03-04T10:34:03.521079Z","shell.execute_reply.started":"2024-03-04T10:34:02.496292Z","shell.execute_reply":"2024-03-04T10:34:03.519461Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# [1, 197, 768]\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"],"ename":"TypeError","evalue":"'NoneType' object is not subscriptable","output_type":"error"}]},{"cell_type":"code","source":"outputs.last_hidden_state[:,0,:].shape","metadata":{"execution":{"iopub.status.busy":"2024-03-01T06:48:40.095341Z","iopub.execute_input":"2024-03-01T06:48:40.095760Z","iopub.status.idle":"2024-03-01T06:48:40.104141Z","shell.execute_reply.started":"2024-03-01T06:48:40.095732Z","shell.execute_reply":"2024-03-01T06:48:40.102770Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 768])"},"metadata":{}}]},{"cell_type":"code","source":"# dir(outputs)\n# [batch, channel, height, width]\n# print(inputs['pixel_values'].shape) # torch.Size([1, 3, 224, 224])\n# inputs['pixel_values']\nprint(inputs['pixel_values'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-03-01T05:49:39.109577Z","iopub.execute_input":"2024-03-01T05:49:39.111047Z","iopub.status.idle":"2024-03-01T05:49:39.118061Z","shell.execute_reply.started":"2024-03-01T05:49:39.110999Z","shell.execute_reply":"2024-03-01T05:49:39.116733Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"tensor(0.0373)\n","output_type":"stream"}]},{"cell_type":"code","source":"# load ViT model from huggingface.\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate audio embeddings","metadata":{}},{"cell_type":"code","source":"# now, audio tokens.\n\n# load audio dataset, or just use your own random data.\n\n# load audio feature extractor, \"AST\" model, or WhisperFeatureExtractor\n\n# load model for audio classification or ASTModel.\n\nfrom transformers import AutoProcessor, ASTModel\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\ndataset = dataset.sort(\"id\")\nsampling_rate = dataset.features[\"audio\"].sampling_rate\n\nprocessor = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\nmodel = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n\n# audio file is decoded on the fly\ninputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n\n# modify the length of the input\n\ninputs.input_values=inputs.input_values[:,:512,:]\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n\n# the input state will be padded, if it is too short.\nlist(last_hidden_states.shape),list(inputs.input_values.shape) # [1, 1214, 768]\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:33:37.523797Z","iopub.execute_input":"2024-03-04T10:33:37.524232Z","iopub.status.idle":"2024-03-04T10:33:42.876647Z","shell.execute_reply.started":"2024-03-04T10:33:37.524198Z","shell.execute_reply":"2024-03-04T10:33:42.875608Z"},"scrolled":true,"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"([1, 1214, 768], [1, 512, 128])"},"metadata":{}}]},{"cell_type":"code","source":"# outputs.keys() # ordered dict, ['last_hidden_state', 'pooler_output']\n\noutputs['pooler_output'].shape # [1, 768], ready for audio embedding and classification","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:34:24.715728Z","iopub.execute_input":"2024-03-04T10:34:24.716160Z","iopub.status.idle":"2024-03-04T10:34:24.724874Z","shell.execute_reply.started":"2024-03-04T10:34:24.716125Z","shell.execute_reply":"2024-03-04T10:34:24.723546Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 768])"},"metadata":{}}]},{"cell_type":"code","source":"outputs.last_hidden_state.shape # torch.Size([1, 1214, 768])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:34:21.456735Z","iopub.execute_input":"2024-03-04T10:34:21.457305Z","iopub.status.idle":"2024-03-04T10:34:21.467792Z","shell.execute_reply.started":"2024-03-04T10:34:21.457265Z","shell.execute_reply":"2024-03-04T10:34:21.466046Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 1214, 768])"},"metadata":{}}]},{"cell_type":"code","source":"type(processor), type(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T07:28:31.459090Z","iopub.execute_input":"2024-03-01T07:28:31.459538Z","iopub.status.idle":"2024-03-01T07:28:31.467041Z","shell.execute_reply.started":"2024-03-01T07:28:31.459506Z","shell.execute_reply":"2024-03-01T07:28:31.465641Z"},"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"(transformers.models.audio_spectrogram_transformer.feature_extraction_audio_spectrogram_transformer.ASTFeatureExtractor,\n transformers.models.audio_spectrogram_transformer.modeling_audio_spectrogram_transformer.ASTModel)"},"metadata":{}}]},{"cell_type":"code","source":"dataset[0]['audio']['array'].shape, inputs.input_values.shape \n\n# 1024 is the max length of the fbank transform.\n\n\n# ((93680,), torch.Size([1, 1024, 128]))\n# independent of audio length? ","metadata":{"execution":{"iopub.status.busy":"2024-03-01T07:38:35.739454Z","iopub.execute_input":"2024-03-01T07:38:35.739957Z","iopub.status.idle":"2024-03-01T07:38:35.761429Z","shell.execute_reply.started":"2024-03-01T07:38:35.739915Z","shell.execute_reply":"2024-03-01T07:38:35.759937Z"},"trusted":true},"execution_count":129,"outputs":[{"execution_count":129,"output_type":"execute_result","data":{"text/plain":"((93680,), torch.Size([1, 1024, 128]))"},"metadata":{}}]},{"cell_type":"code","source":"# accepts at most 10.24 seconds of audio.\n# so we can deduce: max_length/100 = max_audio_length_in_seconds\n\n# window_shift = int(sample_frequency * frame_shift / 1000)\n# m = num_samples / window_shift (approximately)\n\n# to ensure consistency you might want to pad or truncate fbank output \n\n# PS: WhisperFeatureExtractor uses chunk_length=30 to limit input to 30 seconds.\n# Whisper model is bounded to 30 second inputs. Shorter inputs need to be padded.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get fbank actual length\n\nimport torchaudio\n\nwaveform = torch.from_numpy(dataset[0]['audio']['array']).unsqueeze(0)\nsample_frequency = processor.sampling_rate\nnum_mel_bins = processor.num_mel_bins\n\nfbank = torchaudio.compliance.kaldi.fbank(waveform, sample_frequency = sample_frequency, window_type='hanning', num_mel_bins = num_mel_bins)\nfbank.shape # torch.Size([584, 128])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-01T07:59:27.338589Z","iopub.execute_input":"2024-03-01T07:59:27.339020Z","iopub.status.idle":"2024-03-01T07:59:27.366562Z","shell.execute_reply.started":"2024-03-01T07:59:27.338988Z","shell.execute_reply":"2024-03-01T07:59:27.365326Z"},"trusted":true},"execution_count":133,"outputs":[{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"torch.Size([584, 128])"},"metadata":{}}]},{"cell_type":"code","source":"# to run multilingual transcription, first slice the audio by speakers, then run whisper on each segment\n# https://github.com/pyannote/pyannote-audio (speaker diarization)\n\n# For audio segmentation and classification, check You-Only-Hear-Once\n# So we can differentiate speech, music or noise","metadata":{},"execution_count":null,"outputs":[]}]}